### Dealing with consistency issues

One problem arose during the refactoring when designing the file system in the new domain model, i.e. the files and folders structure. In the original design, the file system model was achieved by Hibernate file and folder entities that referenced other entities of the same type to form a tree of relationships. These entities were supposed to be refactored as well. 

By the definition, an aggregate is a set of related objects that represents a transactional boundary. This was considered in the refactoring of the entities. There are some business rules that had to be preserved when designing aggregates for the file system. The first one is consistency of the delete operation on a folder, the second is uniqueness of node name in a folder. These business rules could be uphold by modeling the file system as one aggregate, containing folders and files in tree hierarchy. However, the folders can be nested of course, so the aggregate system could grow very large and loading an aggregate with thousands of objects would be incredibly resource intensive and impossible to scale. In **reference needed**(https://vaughnvernon.co/wordpress/wp-content/uploads/2014/10/DDD_COMMUNITY_ESSAY_AGGREGATES_PART_1.pdf )... it is advised to make aggregates small, also it provides solutions to consistency issues, i.e. by using eventual consistency. So the folders and files were designed as self-contained aggregates. Similarly to the original entities, the two new aggregates, `Folder` and `File`, shared some common functionality that was abstracted to a parent class (`AbstractNodeAggregateRoot`).

As the individual instances of the `Folder` aggregate have no access to the names of the contained files and subfolders, it was needed to find another way how to ensure uniqueness of node names in a folder. To resolve this issue, a special read model is built by events to store only identifiers of nodes and their parents and names. This model is queried just before creating a new child node, renaming a node, or moving a node to other folder to ensure uniqueness of names in that particular folder. Read model is also kept in sync accordingly to the changes made to the nodes. There is no consistency issue here, because building the read model, in this case, happens in the same transaction as persisting changes of an aggregate instance.

The other problem was the delete operation on a folder. In the original design, when a folder was deleted, it recursively deleted all the child folders and files in one transaction via Hibernate's support of collections. Now, that a folder is represented by an aggregate which doesn't hold references to other child folders or files, it wasn't easy to do the recursive deletion of child nodes. This issue was difficult to solve without a lot of experience with CQRS design. But it then occured that a saga (or in this case, a process manager) could be of use. This idea was validated by CQRS practioners in **citation needed**(http://programmers.stackexchange.com/questions/298462/deleting-subtrees-in-hierarchical-agreggates).

The basic principle is that the process manager (called `FolderDeletionSaga` in the code) coordinates the deletion of multiple aggregate instances representing the child nodes of a folder. It is started by publishing event `FolderDeletionStartedEvent` by the `Folder` aggregate instance, which marks the beginning of the deletion process. Using the same consistent read model as before, it initially queries for the children of the folder to delete. For each child node (folder or file) it sends a new command to the respective aggregate instance to delete itself first. File aggregate instances are deleted trivially. For each child folder, however, a new process manager is recursively instantiated to manage the deletion process of that folder. When all the nodes are deleted from a folder, i.e. the folder is empty, the saga managing the folder deletion sends one final internal command to the folder aggregate instance that marks it deleted. When the top saga instance, that initiated the recursive deletion process, is reached, the process is finished. Because the sagas are driven by events published by the aggregates, it is very easy to track the state of the deletion process.

